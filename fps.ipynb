{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from torch_cluster import fps\n",
    "import torch\n",
    "import os,sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coords', 'feats', 'sem_label', '1024_coords', '2048_coords', '4096_coords', '1024_feats', '2048_feats', '4096_feats', '1024_sem_label', '2048_sem_label', '4096_sem_label', 'scene_name', 'scene_label', '256_coords', '256_feats', '256_sem_label', '512_coords', '512_feats', '512_sem_label', '128_coords', '128_feats', '128_sem_label', '32_coords', '32_feats', '32_sem_label', '64_coords', '64_feats', '64_sem_label', '8192_coords', '8192_feats', '8192_sem_label', '16184_coords', '16184_feats', '16184_sem_label', '1_coords', '1_feats', '1_sem_label', '0_coords', '0_feats', '0_sem_label'])\n"
     ]
    }
   ],
   "source": [
    "path='tmp/scene0000_00_vh_clean_2.pth'\n",
    "data=torch.load(path)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords <class 'numpy.ndarray'>\n",
      "feats <class 'numpy.ndarray'>\n",
      "sem_label <class 'numpy.ndarray'>\n",
      "1024_coords <class 'numpy.ndarray'>\n",
      "2048_coords <class 'numpy.ndarray'>\n",
      "4096_coords <class 'numpy.ndarray'>\n",
      "1024_feats <class 'numpy.ndarray'>\n",
      "2048_feats <class 'numpy.ndarray'>\n",
      "4096_feats <class 'numpy.ndarray'>\n",
      "1024_sem_label <class 'numpy.ndarray'>\n",
      "2048_sem_label <class 'numpy.ndarray'>\n",
      "4096_sem_label <class 'numpy.ndarray'>\n",
      "scene_name <class 'str'>\n",
      "scene_label <class 'int'>\n",
      "256_coords <class 'numpy.ndarray'>\n",
      "256_feats <class 'numpy.ndarray'>\n",
      "256_sem_label <class 'numpy.ndarray'>\n",
      "512_coords <class 'numpy.ndarray'>\n",
      "512_feats <class 'numpy.ndarray'>\n",
      "512_sem_label <class 'numpy.ndarray'>\n",
      "128_coords <class 'numpy.ndarray'>\n",
      "128_feats <class 'numpy.ndarray'>\n",
      "128_sem_label <class 'numpy.ndarray'>\n",
      "32_coords <class 'numpy.ndarray'>\n",
      "32_feats <class 'numpy.ndarray'>\n",
      "32_sem_label <class 'numpy.ndarray'>\n",
      "64_coords <class 'numpy.ndarray'>\n",
      "64_feats <class 'numpy.ndarray'>\n",
      "64_sem_label <class 'numpy.ndarray'>\n",
      "8192_coords <class 'numpy.ndarray'>\n",
      "8192_feats <class 'numpy.ndarray'>\n",
      "8192_sem_label <class 'numpy.ndarray'>\n",
      "16184_coords <class 'numpy.ndarray'>\n",
      "16184_feats <class 'numpy.ndarray'>\n",
      "16184_sem_label <class 'numpy.ndarray'>\n",
      "1_coords <class 'numpy.ndarray'>\n",
      "1_feats <class 'numpy.ndarray'>\n",
      "1_sem_label <class 'numpy.ndarray'>\n",
      "0_coords <class 'numpy.ndarray'>\n",
      "0_feats <class 'numpy.ndarray'>\n",
      "0_sem_label <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key,type(data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=torch.float\n",
    "device=torch.device('cuda:0')\n",
    "def mFPS(in_path,out_path,expect_num,device,dtype):\n",
    "    pcd=o3d.io.read_point_cloud(in_path)\n",
    "    dense_pts=torch.tensor(pcd.points,dtype=dtype,device=device)\n",
    "    n_pts=dense_pts.size()[0]\n",
    "    ratio=expect_num/n_pts*1.0\n",
    "    sparse_indice=fps(dense_pts,ratio=ratio,random_start=False)\n",
    "    if(sparse_indice.size()[0]!=expect_num):\n",
    "        sparse_indice=sparse_indice[list(np.linspace(0,sparse_indice.size()[0],expect_num,endpoint=False,dtype=int))]\n",
    "    sparse_indice=sparse_indice.tolist()\n",
    "    assert len(sparse_indice)==expect_num\n",
    "    \n",
    "    # sparse pcd\n",
    "    sparse_points=np.asarray(pcd.points)[sparse_indice]\n",
    "    sparse_colors=np.asarray(pcd.colors)[sparse_indice]\n",
    "    pcd.points=o3d.utility.Vector3dVector()\n",
    "    pcd.colors=o3d.utility.Vector3dVector()\n",
    "    pcd.points=o3d.utility.Vector3dVector(sparse_points)\n",
    "    pcd.colors=o3d.utility.Vector3dVector(sparse_colors)\n",
    "    \n",
    "    # write\n",
    "    assert o3d.io.write_point_cloud(out_path, pcd)==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General ScanNet info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reconstructed surface mesh file (`*.ply`)**:\n",
    "Binary PLY format mesh with +Z axis in upright orientation.\n",
    "\n",
    "- **RGB-D sensor stream (`*.sens`)**:\n",
    "Compressed binary format with per-frame color, depth, camera pose and other data.  See [ScanNet C++ Toolkit](#scannet-c-toolkit) for more information and parsing code. See [SensReader/python](SensReader/python) for a very basic python data exporter.\n",
    "\n",
    "- **Surface mesh segmentation file (`*.segs.json`)**:\n",
    "```javascript\n",
    "{\n",
    "  \"params\": {  // segmentation parameters\n",
    "   \"kThresh\": \"0.0001\",\n",
    "   \"segMinVerts\": \"20\",\n",
    "   \"minPoints\": \"750\",\n",
    "   \"maxPoints\": \"30000\",\n",
    "   \"thinThresh\": \"0.05\",\n",
    "   \"flatThresh\": \"0.001\",\n",
    "   \"minLength\": \"0.02\",\n",
    "   \"maxLength\": \"1\"\n",
    "  },\n",
    "  \"sceneId\": \"...\",  // id of segmented scene\n",
    "  \"segIndices\": [1,1,1,1,3,3,15,15,15,15],  // per-vertex index of mesh segment\n",
    "}\n",
    "```\n",
    "\n",
    "- **Aggregated semantic annotation file (`*.aggregation.json`)**:\n",
    "```javascript\n",
    "{\n",
    "  \"sceneId\": \"...\",  // id of annotated scene\n",
    "  \"appId\": \"...\", // id + version of the tool used to create the annotation\n",
    "  \"segGroups\": [\n",
    "    {\n",
    "      \"id\": 0,\n",
    "      \"objectId\": 0,\n",
    "      \"segments\": [1,4,3],\n",
    "      \"label\": \"couch\"\n",
    "    },\n",
    "  ],\n",
    "  \"segmentsFile\": \"...\" // id of the *.segs.json segmentation file referenced\n",
    "}\n",
    "```\n",
    "[BenchmarkScripts/util_3d.py](BenchmarkScripts/util_3d.py) gives examples to parsing the semantic instance information from the `*.segs.json`, `*.aggregation.json`, and `*_vh_clean_2.ply` mesh file, with example semantic segmentation visualization in [BenchmarkScripts/3d_helpers/visualize_labels_on_mesh.py](BenchmarkScripts/3d_helpers/visualize_labels_on_mesh.py).\n",
    "\n",
    "- **2d annotation projections (`*_2d-label.zip`, `*_2d-instance.zip`, `*_2d-label-filt.zip`, `*_2d-instance-filt.zip`)**:\n",
    "Projection of 3d aggregated annotation of a scan into its RGB-D frames, according to the computed camera trajectory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
